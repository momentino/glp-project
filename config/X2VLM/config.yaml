## Vision Encoder
use_beit_v2: True
vision_config: '../models/X2VLM/configs/config_beit2_base.json'
image_res: 224
patch_size: 16
local_attn_depth: -1


## Text Encoder (& Cross Encoder)
text_encoder: 'bert-base-uncased'
text_num_hidden_layers: 18  # include cross
text_fusion_start_at: 12


## Training
mixed_in_batch: True
calc_image_bbox_loss: False
embed_dim: 256
temp: 0.07

max_words: 30
max_tokens: 30
mask_prob: 0.5
max_masks: 12
mask_whole_word: True
skipgram_prb: 0.2
skipgram_size: 3

stop_calc_itm: 200000  # steps; matching loss calculates hard negatives causing nan loss


## Other Settings
ckpt_frequent_step: 50000
ckpt_frequent: 100000000  # inf
optimizer: {opt: adamW, lr: 1e-4, weight_decay: 0.01, lr_mult: 2}
schedular: {sched: linear, lr: 1e-4, epochs: 3, num_warmup_steps: 2500}
accelerator: {SYNCBN: false, FP16_OPT_LEVEL: O1, FP16_LOSS_SCALE: dynamic, RNG_SEED: 42, GRAD_ACCUMULATE_STEPS: 1, CLIP_GRAD_NORM: 1.0}

pretrained_weights: ../pretrained_weights/X2VLM_weights.pth







